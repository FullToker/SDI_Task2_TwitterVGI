{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between ML & LLM methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "import openai\n",
    "import re\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import tiktoken  # For token counting\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EurovisionTweetAnalyzer:\n",
    "    def __init__(self, openai_api_key: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the analyzer\n",
    "        \n",
    "        Args:\n",
    "            openai_api_key: OpenAI API key\n",
    "        \"\"\"\n",
    "        if openai_api_key:\n",
    "            openai.api_key = openai_api_key\n",
    "            self.openai_client = openai.OpenAI(api_key=openai_api_key)\n",
    "        \n",
    "        # Initialize tiktoken encoder for token counting\n",
    "        self.encoder = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    def load_tweets(self, json_file_path: str) -> List[Dict]:\n",
    "        \"\"\"Load Twitter JSON data\"\"\"\n",
    "        try:\n",
    "            with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # If it's a single object, convert to list\n",
    "            if isinstance(data, dict):\n",
    "                data = [data]\n",
    "            \n",
    "            logger.info(f\"Successfully loaded {len(data)} tweets\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load JSON file: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in a text string\"\"\"\n",
    "        tokens = self.encoder.encode(text)\n",
    "        return len(tokens)\n",
    "    \n",
    "    def analyze_with_chatgpt(self, tweet_text: str, tweet_metadata: Dict = None) -> Dict:\n",
    "        \"\"\"Analyze tweet with ChatGPT and track time and token usage\"\"\"\n",
    "        try:\n",
    "            # Create the prompt\n",
    "            prompt = f\"\"\"\n",
    "            Please analyze if the following tweet is related to the Eurovision Song Contest, and provide detailed analysis:\n",
    "\n",
    "            Tweet content: \"{tweet_text}\"\n",
    "\n",
    "            Please return results in this JSON format:\n",
    "            {{\n",
    "                \"is_eurovision_related\": true/false,\n",
    "                \"confidence_score\": 0.0-1.0,\n",
    "                \"detected_language\": \"language code\",\n",
    "                \"english_translation\": \"English translation (if needed)\",\n",
    "                \"eurovision_keywords\": [\"relevant keyword list\"],\n",
    "                \"location_mentions\": [\"mentioned locations\"],\n",
    "                \"sentiment\": \"positive/negative/neutral\",\n",
    "                \"sentiment_score\": 0.0-1.0,\n",
    "                \"reasoning\": \"reasoning for judgment\"\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            # Count prompt tokens\n",
    "            system_message = \"You are an expert in analyzing Eurovision-related content. Please accurately determine if tweets are related to the Eurovision Song Contest.\"\n",
    "            prompt_tokens = self.count_tokens(system_message) + self.count_tokens(prompt)\n",
    "            \n",
    "            # Track start time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Make API call\n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_message},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1\n",
    "            )\n",
    "            \n",
    "            # Calculate processing time\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # Extract token usage from response\n",
    "            completion_tokens = response.usage.completion_tokens\n",
    "            prompt_tokens_reported = response.usage.prompt_tokens\n",
    "            total_tokens = response.usage.total_tokens\n",
    "            \n",
    "            # Parse result\n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "            \n",
    "            # Add metadata about the API call\n",
    "            result['api_used'] = 'chatgpt'\n",
    "            result['processing_time_seconds'] = processing_time\n",
    "            result['prompt_tokens'] = prompt_tokens_reported\n",
    "            result['completion_tokens'] = completion_tokens\n",
    "            result['total_tokens'] = total_tokens\n",
    "            result['estimated_prompt_tokens'] = prompt_tokens  # Our pre-call estimate\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"ChatGPT analysis failed: {e}\")\n",
    "            return {\n",
    "                \"error\": str(e), \n",
    "                \"api_used\": \"chatgpt\",\n",
    "                \"processing_time_seconds\": time.time() - start_time if 'start_time' in locals() else None\n",
    "            }\n",
    "    \n",
    "    def traditional_keyword_filter(self, tweet_text: str) -> Dict:\n",
    "        \"\"\"Traditional keyword filtering method with time tracking\"\"\"\n",
    "        # Track start time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        eurovision_keywords = [\n",
    "            'eurovision'\n",
    "        ]\n",
    "        \n",
    "        # Participating country keywords\n",
    "        countries = [\n",
    "            'sweden', 'ukraine', 'italy', 'netherlands', 'spain',\n",
    "            'germany', 'france', 'united kingdom', 'australia',\n",
    "            'israel', 'norway', 'finland', 'denmark', 'iceland'\n",
    "        ]\n",
    "        \n",
    "        text_lower = tweet_text.lower()\n",
    "        found_keywords = []\n",
    "        found_countries = []\n",
    "        \n",
    "        for keyword in eurovision_keywords:\n",
    "            if keyword in text_lower:\n",
    "                found_keywords.append(keyword)\n",
    "        \n",
    "        for country in countries:\n",
    "            if country in text_lower:\n",
    "                found_countries.append(country)\n",
    "        \n",
    "        is_related = len(found_keywords) > 0 or len(found_countries) > 1\n",
    "        confidence = min(1.0, (len(found_keywords) + len(found_countries) * 0.5) / 3)\n",
    "        \n",
    "        # Calculate processing time\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            \"is_eurovision_related\": is_related,\n",
    "            \"confidence_score\": confidence,\n",
    "            \"eurovision_keywords\": found_keywords,\n",
    "            \"location_mentions\": found_countries,\n",
    "            \"method\": \"traditional_keywords\",\n",
    "            \"processing_time_seconds\": processing_time\n",
    "        }\n",
    "    \n",
    "    def batch_analyze(self, tweets: List[Dict], method: str = \"both\", \n",
    "                     save_results: bool = True, output_file: str = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Batch analyze tweets\n",
    "        \n",
    "        Args:\n",
    "            tweets: List of tweet data\n",
    "            method: Analysis method (\"chatgpt\", \"both\", \"traditional\")\n",
    "            save_results: Whether to save results\n",
    "            output_file: Output file path\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Track overall processing time\n",
    "        batch_start_time = time.time()\n",
    "        \n",
    "        # Track token and time usage for summary\n",
    "        api_tokens_used = 0\n",
    "        api_time_total = 0\n",
    "        traditional_time_total = 0\n",
    "        tweets_analyzed = 0\n",
    "        \n",
    "        for i, tweet in enumerate(tweets):\n",
    "            try:\n",
    "                # Extract tweet text\n",
    "                tweet_text = tweet.get('text', tweet.get('full_text', ''))\n",
    "                if not tweet_text:\n",
    "                    continue\n",
    "                \n",
    "                tweets_analyzed += 1\n",
    "                logger.info(f\"Processing tweet {i+1}/{len(tweets)}\")\n",
    "                \n",
    "                result = {\n",
    "                    'tweet_id': tweet.get('id_str', tweet.get('id', i)),\n",
    "                    'tweet_text': tweet_text,\n",
    "                    'user_screen_name': tweet.get('user', {}).get('screen_name', ''),\n",
    "                    'created_at': tweet.get('created_at', ''),\n",
    "                    'original_tweet': tweet\n",
    "                }\n",
    "                \n",
    "                # Analyze based on selected method\n",
    "                if method in [\"chatgpt\", \"both\"]:\n",
    "                    try:\n",
    "                        chatgpt_result = self.analyze_with_chatgpt(tweet_text, tweet)\n",
    "                        result['chatgpt_analysis'] = chatgpt_result\n",
    "                        \n",
    "                        # Track API usage statistics\n",
    "                        if 'total_tokens' in chatgpt_result:\n",
    "                            api_tokens_used += chatgpt_result['total_tokens']\n",
    "                        if 'processing_time_seconds' in chatgpt_result:\n",
    "                            api_time_total += chatgpt_result['processing_time_seconds']\n",
    "                            \n",
    "                        time.sleep(1)  # API rate limiting\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"ChatGPT analysis failed: {e}\")\n",
    "                \n",
    "                if method in [\"traditional\", \"both\"]:\n",
    "                    traditional_result = self.traditional_keyword_filter(tweet_text)\n",
    "                    result['traditional_analysis'] = traditional_result\n",
    "                    \n",
    "                    # Track traditional method time\n",
    "                    if 'processing_time_seconds' in traditional_result:\n",
    "                        traditional_time_total += traditional_result['processing_time_seconds']\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing tweet {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        batch_processing_time = time.time() - batch_start_time\n",
    "        \n",
    "        # Create summary statistics\n",
    "        analysis_summary = {\n",
    "            'total_tweets_analyzed': tweets_analyzed,\n",
    "            'batch_processing_time_seconds': batch_processing_time,\n",
    "            'api_total_tokens_used': api_tokens_used,\n",
    "            'api_average_tokens_per_tweet': api_tokens_used / tweets_analyzed if tweets_analyzed > 0 and api_tokens_used > 0 else 0,\n",
    "            'api_total_processing_time_seconds': api_time_total,\n",
    "            'api_average_time_per_tweet_seconds': api_time_total / tweets_analyzed if tweets_analyzed > 0 and api_time_total > 0 else 0,\n",
    "            'traditional_total_processing_time_seconds': traditional_time_total,\n",
    "            'traditional_average_time_per_tweet_seconds': traditional_time_total / tweets_analyzed if tweets_analyzed > 0 and traditional_time_total > 0 else 0,\n",
    "        }\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.json_normalize(results)\n",
    "        \n",
    "        # Add summary as metadata\n",
    "        df.attrs['analysis_summary'] = analysis_summary\n",
    "        \n",
    "        # Print summary\n",
    "        logger.info(\"\\n===== Analysis Summary =====\")\n",
    "        for key, value in analysis_summary.items():\n",
    "            if isinstance(value, float):\n",
    "                logger.info(f\"{key}: {value:.4f}\")\n",
    "            else:\n",
    "                logger.info(f\"{key}: {value}\")\n",
    "        \n",
    "        # Save results\n",
    "        if save_results:\n",
    "            if not output_file:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                output_file = f\"eurovision_analysis_{timestamp}.csv\"\n",
    "            \n",
    "            df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            # Save summary to separate file\n",
    "            summary_file = output_file.replace('.csv', '_summary.json')\n",
    "            with open(summary_file, 'w') as f:\n",
    "                json.dump(analysis_summary, f, indent=2)\n",
    "                \n",
    "            logger.info(f\"Results saved to: {output_file}\")\n",
    "            logger.info(f\"Summary saved to: {summary_file}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def compare_methods(self, results_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Compare different methods' results\"\"\"\n",
    "        comparison = {}\n",
    "        \n",
    "        if 'chatgpt_analysis.is_eurovision_related' in results_df.columns:\n",
    "            chatgpt_positive = results_df['chatgpt_analysis.is_eurovision_related'].sum()\n",
    "            chatgpt_tokens = results_df['chatgpt_analysis.total_tokens'].sum()\n",
    "            chatgpt_time = results_df['chatgpt_analysis.processing_time_seconds'].sum()\n",
    "            \n",
    "            comparison['chatgpt'] = {\n",
    "                'total_analyzed': len(results_df),\n",
    "                'eurovision_related': chatgpt_positive,\n",
    "                'percentage': chatgpt_positive / len(results_df) * 100,\n",
    "                'total_tokens_used': chatgpt_tokens,\n",
    "                'average_tokens_per_tweet': chatgpt_tokens / len(results_df),\n",
    "                'total_processing_time': chatgpt_time,\n",
    "                'average_time_per_tweet': chatgpt_time / len(results_df)\n",
    "            }\n",
    "        \n",
    "        if 'traditional_analysis.is_eurovision_related' in results_df.columns:\n",
    "            traditional_positive = results_df['traditional_analysis.is_eurovision_related'].sum()\n",
    "            traditional_time = results_df['traditional_analysis.processing_time_seconds'].sum()\n",
    "            \n",
    "            comparison['traditional'] = {\n",
    "                'total_analyzed': len(results_df),\n",
    "                'eurovision_related': traditional_positive,\n",
    "                'percentage': traditional_positive / len(results_df) * 100,\n",
    "                'total_processing_time': traditional_time,\n",
    "                'average_time_per_tweet': traditional_time / len(results_df)\n",
    "            }\n",
    "        \n",
    "        # Add agreement statistics if both methods were used\n",
    "        if 'chatgpt_analysis.is_eurovision_related' in results_df.columns and 'traditional_analysis.is_eurovision_related' in results_df.columns:\n",
    "            agreement = (results_df['chatgpt_analysis.is_eurovision_related'] == results_df['traditional_analysis.is_eurovision_related']).sum()\n",
    "            \n",
    "            comparison['comparison'] = {\n",
    "                'agreement_count': agreement,\n",
    "                'agreement_percentage': agreement / len(results_df) * 100,\n",
    "                'time_difference_factor': comparison['chatgpt']['average_time_per_tweet'] / comparison['traditional']['average_time_per_tweet'] if comparison['traditional']['average_time_per_tweet'] > 0 else float('inf')\n",
    "            }\n",
    "        \n",
    "        return comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully loaded 179983 tweets\n",
      "INFO:__main__:Processing tweet 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始分析推文...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 2/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 3/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 4/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 5/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 6/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 7/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 8/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 9/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 10/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "ERROR:__main__:ChatGPT analysis failed: Invalid control character at: line 5 column 50 (char 147)\n",
      "INFO:__main__:Processing tweet 11/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 12/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 13/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 14/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 15/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 16/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 17/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 18/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 19/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 20/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 21/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 22/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 23/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 24/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 25/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 26/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 27/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 28/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 29/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 30/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 31/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "ERROR:__main__:ChatGPT analysis failed: Expecting value: line 1 column 1 (char 0)\n",
      "INFO:__main__:Processing tweet 32/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 33/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 34/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 35/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 36/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 37/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 38/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 39/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 40/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 41/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 42/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 43/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 44/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 45/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 46/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 47/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 48/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 49/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 50/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 51/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 52/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 53/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 54/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 55/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 56/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 57/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 58/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 59/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 60/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 61/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 62/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 63/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 64/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 65/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 66/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 67/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 68/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 69/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 70/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 71/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 72/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 73/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 74/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 75/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 76/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 77/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "ERROR:__main__:ChatGPT analysis failed: Expecting value: line 1 column 1 (char 0)\n",
      "INFO:__main__:Processing tweet 78/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 79/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 80/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 81/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 82/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 83/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 84/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 85/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 86/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 87/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 88/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 89/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 90/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 91/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 92/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 93/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "ERROR:__main__:ChatGPT analysis failed: Expecting value: line 1 column 1 (char 0)\n",
      "INFO:__main__:Processing tweet 94/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 95/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 96/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 97/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "ERROR:__main__:ChatGPT analysis failed: Expecting value: line 1 column 1 (char 0)\n",
      "INFO:__main__:Processing tweet 98/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 99/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing tweet 100/100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:\n",
      "===== Analysis Summary =====\n",
      "INFO:__main__:total_tweets_analyzed: 100\n",
      "INFO:__main__:batch_processing_time_seconds: 775.4824\n",
      "INFO:__main__:api_total_tokens_used: 32763\n",
      "INFO:__main__:api_average_tokens_per_tweet: 327.6300\n",
      "INFO:__main__:api_total_processing_time_seconds: 675.3580\n",
      "INFO:__main__:api_average_time_per_tweet_seconds: 6.7536\n",
      "INFO:__main__:traditional_total_processing_time_seconds: 0.0013\n",
      "INFO:__main__:traditional_average_time_per_tweet_seconds: 0.0000\n",
      "INFO:__main__:Results saved to: eurovision_analysis_20250624_213019.csv\n",
      "INFO:__main__:Summary saved to: eurovision_analysis_20250624_213019_summary.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 分析结果对比 ===\n",
      "CHATGPT:\n",
      "  总分析数量: 100\n",
      "  Eurovision相关: 6\n",
      "  相关比例: 6.00%\n",
      "  总Token消耗: 32763.0\n",
      "  平均每条Token: 327.63\n",
      "  总处理时间: 675.36秒\n",
      "  平均每条处理时间: 6.7536秒\n",
      "\n",
      "TRADITIONAL:\n",
      "  总分析数量: 100\n",
      "  Eurovision相关: 6\n",
      "  相关比例: 6.00%\n",
      "  总处理时间: 0.00秒\n",
      "  平均每条处理时间: 0.0000秒\n",
      "\n",
      "两种方法比较:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'total_analyzed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m comp \u001b[38;5;241m=\u001b[39m comparison[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomparison\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m两种方法比较:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  一致性: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magreement_count\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_analyzed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magreement_percentage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  API方法比传统方法慢: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_difference_factor\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m倍\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'total_analyzed'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 配置API密钥\n",
    "OPENAI_API_KEY = \"\"  # 替换为你的OpenAI API密钥\n",
    "    \n",
    "    # 初始化分析器\n",
    "analyzer = EurovisionTweetAnalyzer(\n",
    "        openai_api_key=OPENAI_API_KEY\n",
    "    )\n",
    "    \n",
    "tweets = analyzer.load_tweets(\"/media/ys_tum/T7 Shield/25SS/SDI_data/tweets_europe_west_2016_05_10.json\")  # json path\n",
    "if not tweets:\n",
    "    print(\"没有加载到推文数据\")\n",
    "    raise SystemExit(\"程序终止：没有加载到推文数据\")\n",
    "    \n",
    "# 选择分析方法和数量（用于测试）\n",
    "sample_tweets = tweets[:100]  # 先测试前10条\n",
    "    \n",
    "print(\"开始分析推文...\")\n",
    "results_df = analyzer.batch_analyze(\n",
    "        sample_tweets, \n",
    "        method=\"both\",  # 使用两种LLM方法\n",
    "        save_results=True\n",
    "    )\n",
    "    \n",
    "    # 对比结果\n",
    "comparison = analyzer.compare_methods(results_df)\n",
    "\n",
    "print(\"\\n=== 分析结果对比 ===\")\n",
    "for method, stats in comparison.items():\n",
    "    if method == \"comparison\":\n",
    "        continue  # 先跳过比较部分，最后单独显示\n",
    "    \n",
    "    print(f\"{method.upper()}:\")\n",
    "    print(f\"  总分析数量: {stats['total_analyzed']}\")\n",
    "    print(f\"  Eurovision相关: {stats['eurovision_related']}\")\n",
    "    print(f\"  相关比例: {stats['percentage']:.2f}%\")\n",
    "    \n",
    "    # 添加处理时间信息\n",
    "    if method == \"chatgpt\":\n",
    "        print(f\"  总Token消耗: {stats['total_tokens_used']}\")\n",
    "        print(f\"  平均每条Token: {stats['average_tokens_per_tweet']:.2f}\")\n",
    "        print(f\"  总处理时间: {stats['total_processing_time']:.2f}秒\")\n",
    "        print(f\"  平均每条处理时间: {stats['average_time_per_tweet']:.4f}秒\")\n",
    "    elif method == \"traditional\":\n",
    "        print(f\"  总处理时间: {stats['total_processing_time']:.2f}秒\")\n",
    "        print(f\"  平均每条处理时间: {stats['average_time_per_tweet']:.4f}秒\")\n",
    "    print()\n",
    "\n",
    "# 添加方法比较部分\n",
    "if \"comparison\" in comparison:\n",
    "    comp = comparison[\"comparison\"]\n",
    "    print(\"两种方法比较:\")\n",
    "    print(f\"  一致性: {comp['agreement_count']}/{stats['total_analyzed']} ({comp['agreement_percentage']:.2f}%)\")\n",
    "    print(f\"  API方法比传统方法慢: {comp['time_difference_factor']:.2f}倍\")\n",
    "    print()\n",
    "\n",
    "# 显示一些样例结果\n",
    "print(\"=== 样例分析结果 ===\")\n",
    "for i in range(min(3, len(results_df))):\n",
    "    row = results_df.iloc[i]\n",
    "    print(f\"\\n推文 {i+1}: {row['tweet_text'][:100]}...\")\n",
    "    \n",
    "    if 'chatgpt_analysis.is_eurovision_related' in row:\n",
    "        print(f\"ChatGPT判断: {row['chatgpt_analysis.is_eurovision_related']}\")\n",
    "        print(f\"处理时间: {row['chatgpt_analysis.processing_time_seconds']:.4f}秒\")\n",
    "        print(f\"Token消耗: {row['chatgpt_analysis.total_tokens']}\")\n",
    "    \n",
    "    if 'traditional_analysis.is_eurovision_related' in row:\n",
    "        print(f\"传统方法判断: {row['traditional_analysis.is_eurovision_related']}\")\n",
    "        print(f\"处理时间: {row['traditional_analysis.processing_time_seconds']:.4f}秒\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
